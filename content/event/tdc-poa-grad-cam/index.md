---
abstract: Interpretability in Deep Learning contributes to increasing confidence and transparency in systems based on these techniques. Especially in safety-critical systems such as autonomous vehicles or unmanned aerial vehicles. In this talk, we will understand how to explain the predictions of convolutional neural networks (CNNs) using Grad-CAM. December 2020, The Developer's Conference, SÃ£o Paulo - SP, Brazil.
address:
  # city: Porto Alegre
  # country: Brazil
  # postcode: "94305"
  # region: RS
  # street: 450 Serra Mall
all_day: true
authors: []
draft: true
date: "2020-12-03"
# date_end: "2030-06-01"
event: The Developers Conference
event_url: https://thedevconf.com/tdc/2020/poaonline/trilha-inteligencia-artificial-e-machine-learning
featured: false
# image:
#   caption: 'Image credit: [**Unsplash**](https://unsplash.com/photos/bzdhc5b3Bxs)'
#   focal_point: Right
# links:
# - icon: photo-video
#   icon_pack: fa
#   name: Slides
#   url: https://twitter.com/georgecushen
location: Remote
projects:
- internal-project
publishDate: "2020-12-03T00:00:00Z"
# slides: example
summary: A talk on using Grad-CAM to explain CNN predictions and enhance interpretability in safety-critical deep learning systems.
tags: []
title: Demystifying CNNs with Grad-CAM
url_code: ""
url_pdf: ""
url_slides: ""
url_video: ""
---
